# Qwen2-1.5b-Instruct å¾®è°ƒ SwanLabå¯è§†åŒ–è®°å½•ç‰ˆ

æœ¬èŠ‚æˆ‘ä»¬ç®€è¦ä»‹ç»åŸºäº transformersã€peft ç­‰æ¡†æ¶ï¼Œä½¿ç”¨Qwen2-1.5b-Instructæ¨¡å‹åœ¨**zh_cls_fudan-news** ä¸Šè¿›è¡ŒLoraå¾®è°ƒè®­ç»ƒï¼ŒåŒæ—¶ä½¿ç”¨ [SwanLab](https://github.com/swanhubx/swanlab) ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸è¯„ä¼°æ¨¡å‹æ•ˆæœã€‚

å‚è€ƒå·¥ä½œï¼š[Qwen2.5-7B-Instruct Lora å¾®è°ƒ SwanLabå¯è§†åŒ–è®°å½•ç‰ˆ]ï¼ˆhttps://github.com/taurusduan/self-llm-AI/tree/master/models/Qwen2.5ï¼‰
## ç›®å½•

- [ç¯å¢ƒé…ç½®](#-ç¯å¢ƒé…ç½®)
- [å‡†å¤‡æ•°æ®é›†](#-å‡†å¤‡æ•°æ®é›†)
- [æ¨¡å‹å¯è§†åŒ–](#-æ¨¡å‹å¯è§†åŒ–)
- [æ¨¡å‹è®­ç»ƒå¾®è°ƒ](#-æ¨¡å‹è®­ç»ƒå¾®è°ƒ)
- [æ¨ç†æ¨¡å‹ï¼‰](#-æ¨ç†æ¨¡å‹)
- [è®­ç»ƒç»“æœæ¼”ç¤º](#-è®­ç»ƒç»“æœæ¼”ç¤º)
- [è¡¥å……](#è¡¥å……)


## ğŸŒ ç¯å¢ƒé…ç½®

ç¯å¢ƒé…ç½®åˆ†ä¸ºä¸‰æ­¥ï¼š

1. ç¡®ä¿ä½ çš„ç”µè„‘ä¸Šè‡³å°‘æœ‰ä¸€å¼ è‹±ä¼Ÿè¾¾æ˜¾å¡ï¼Œå¹¶å·²å®‰è£…å¥½äº†CUDAç¯å¢ƒã€‚

2. å®‰è£…Pythonï¼ˆç‰ˆæœ¬>=3.8ï¼‰ä»¥åŠèƒ½å¤Ÿè°ƒç”¨CUDAåŠ é€Ÿçš„PyTorchã€‚

3. å®‰è£…å¾®è°ƒç›¸å…³çš„ç¬¬ä¸‰æ–¹åº“ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
pip install swanlab modelscope transformers datasets peft pandas accelerate
```
## å‡†å¤‡æ•°æ®é›†
æœ¬æ¡ˆä¾‹ä½¿ç”¨çš„æ˜¯zh_cls_fudan-newsæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸»è¦è¢«ç”¨äºè®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹ã€‚

zh_cls_fudan-newsç”±å‡ åƒæ¡æ•°æ®ï¼Œæ¯æ¡æ•°æ®åŒ…å«textã€categoryã€outputä¸‰åˆ—ï¼š

text æ˜¯è®­ç»ƒè¯­æ–™ï¼Œå†…å®¹æ˜¯ä¹¦ç±æˆ–æ–°é—»çš„æ–‡æœ¬å†…å®¹
category æ˜¯textçš„å¤šä¸ªå¤‡é€‰ç±»å‹ç»„æˆçš„åˆ—è¡¨
output åˆ™æ˜¯textå”¯ä¸€çœŸå®çš„ç±»å‹

æˆ‘ä»¬çš„è®­ç»ƒä»»åŠ¡ï¼Œä¾¿æ˜¯å¸Œæœ›å¾®è°ƒåçš„å¤§æ¨¡å‹èƒ½å¤Ÿæ ¹æ®Textå’ŒCategoryç»„æˆçš„æç¤ºè¯ï¼Œé¢„æµ‹å‡ºæ­£ç¡®çš„Outputã€‚

æ–‡æœ¬åˆ†ç±»ä»»åŠ¡-æ•°æ®é›†ä¸‹è½½ï¼šåœ¨[swift/zh_cls_fudan-news](https://modelscope.cn/datasets/swift/zh_cls_fudan-news/files)ä¸‹è½½`train.jsonl`å’Œ`test.jsonl`åˆ°æ ¹ç›®å½•ä¸‹ã€‚

å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡-æ•°æ®é›†ä¸‹è½½ï¼šåœ¨[qgyd2021/chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft/tree/main/data)ä¸‹è½½`ccfbdci.jsonl`åˆ°æ ¹ç›®å½•ä¸‹ã€‚

![dataset](./picture/dataset.jpg)

åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œç”¨ä»¥ä¸‹ä»£ç å¤„ç†æ•°æ®é›†ï¼Œå¾—åˆ°æ–°æ•°æ®é›†æ–‡ä»¶ï¼š

```python
import json
def dataset_jsonl_transfer(origin_path, new_path):
    """
    å°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºå¤§æ¨¡å‹å¾®è°ƒæ‰€éœ€æ•°æ®æ ¼å¼çš„æ–°æ•°æ®é›†
    """
    messages = []

    # è¯»å–æ—§çš„JSONLæ–‡ä»¶
    with open(origin_path, "r") as file:
        for line in file:
            # è§£ææ¯ä¸€è¡Œçš„jsonæ•°æ®
            data = json.loads(line)
            context = data["text"]
            catagory = data["category"]
            label = data["output"]
            message = {
                "instruction": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é¢†åŸŸçš„ä¸“å®¶ï¼Œä½ ä¼šæ¥æ”¶åˆ°ä¸€æ®µæ–‡æœ¬å’Œå‡ ä¸ªæ½œåœ¨çš„åˆ†ç±»é€‰é¡¹ï¼Œè¯·è¾“å‡ºæ–‡æœ¬å†…å®¹çš„æ­£ç¡®ç±»å‹",
                "input": f"æ–‡æœ¬:{context},ç±»å‹é€‰å‹:{catagory}",
                "output": label,
            }
            messages.append(message)

    # ä¿å­˜é‡æ„åçš„JSONLæ–‡ä»¶
    with open(new_path, "w", encoding="utf-8") as file:
        for message in messages:
            file.write(json.dumps(message, ensure_ascii=False) + "\n")
```
è‡³æ­¤æˆ‘ä»¬å®Œæˆäº†æ•°æ®é›†å‡†å¤‡


## æ¨¡å‹å¯è§†åŒ–
ä½¿ç”¨SwanLabæ¥ç›‘æ§æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶è¯„ä¼°æœ€ç»ˆçš„æ¨¡å‹æ•ˆæœã€‚

è¿™é‡Œç›´æ¥ä½¿ç”¨SwanLabå’ŒTransformersçš„é›†æˆæ¥å®ç°ã€‚

ç¬¬ä¸€æ¬¡ä½¿ç”¨SwanLabï¼Œè¯·åœ¨ç”¨æˆ·è®¾ç½®é¡µé¢å¤åˆ¶ä½ çš„API Keyï¼šï¼ˆyFJtAXUnspSGfcN5nhHjyï¼‰


```python
from swanlab.integration.huggingface import SwanLabCallback


swanlab_callback = SwanLabCallback(...)

trainer = Trainer(
    ...
    callbacks=[swanlab_callback],
)

```

## æ¨¡å‹è®­ç»ƒ
è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨modelscopeä¸‹è½½Qwen2-1.5B-Instructæ¨¡å‹ï¼Œç„¶åæŠŠå®ƒåŠ è½½åˆ°Transformersä¸­è¿›è¡Œè®­ç»ƒï¼š
| æ¨¡å‹       | ä»»åŠ¡              | è¿è¡Œå‘½ä»¤                                                             | 
| ---------- | ----------------- | -------------------------------------------------------------------- | 
| Qwen2-1.5b | æŒ‡ä»¤å¾®è°ƒ-æ–‡æœ¬åˆ†ç±» | python train_qwen2.py | 
| Qwen2-1.5b    | æŒ‡ä»¤å¾®è°ƒ-å‘½åå®ä½“è¯†åˆ« | python train_qwen2_ner.py | 


## æ¨ç†æ¨¡å‹
è®­å¥½çš„æ¨¡å‹é»˜è®¤è¢«ä¿å­˜åœ¨./output/Qwen2æ–‡ä»¶å¤¹ä¸‹ã€‚é€šè¿‡è¿è¡Œpredict_qwen2.pyæ–‡ä»¶ï¼Œå°†å¾®è°ƒåçš„qwen2æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œé€šè¿‡è¾“å…¥textå’Œcategoryï¼Œè·å¾—æ­£ç¡®çš„outputã€‚å…¶ä»£ç å¦‚ä¸‹ï¼š
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def predict(messages, model, tokenizer):
    device = "cuda"

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return response


# åŠ è½½åŸä¸‹è½½è·¯å¾„çš„tokenizerå’Œmodel
tokenizer = AutoTokenizer.from_pretrained("./qwen/Qwen2-1___5B-Instruct/", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("./qwen/Qwen2-1___5B-Instruct/", device_map="auto", torch_dtype=torch.bfloat16)

# åŠ è½½è®­ç»ƒå¥½çš„Loraæ¨¡å‹ï¼Œå°†ä¸‹é¢çš„checkpointXXXæ›¿æ¢ä¸ºå®é™…çš„checkpointæ–‡ä»¶ååç§°
model = PeftModel.from_pretrained(model, model_id="./output/Qwen2/checkpointXXX")

test_texts = {
    'instruction': "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é¢†åŸŸçš„ä¸“å®¶ï¼Œä½ ä¼šæ¥æ”¶åˆ°ä¸€æ®µæ–‡æœ¬å’Œå‡ ä¸ªæ½œåœ¨çš„åˆ†ç±»é€‰é¡¹ï¼Œè¯·è¾“å‡ºæ–‡æœ¬å†…å®¹çš„æ­£ç¡®ç±»å‹",
    'input': "æ–‡æœ¬:èˆªç©ºåŠ¨åŠ›å­¦æŠ¥JOURNAL OF AEROSPACE POWER1998å¹´ ç¬¬4æœŸ No.4 1998ç§‘æŠ€æœŸåˆŠç®¡è·¯ç³»ç»Ÿæ•·è®¾çš„å¹¶è¡Œå·¥ç¨‹æ¨¡å‹ç ”ç©¶*é™ˆå¿—è‹±*ã€€*ã€€é©¬ã€€æšåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ã€æ‘˜è¦ã€‘ã€€æå‡ºäº†ä¸€ç§åº”ç”¨äºå¹¶è¡Œå·¥ç¨‹æ¨¡å‹è½¬æ¢ç ”ç©¶çš„æ ‡å·æ³•ï¼Œè¯¥æ³•æ˜¯å°†ç°è¡Œä¸²è¡Œè®¾è®¡è¿‡ç¨‹(As-is)è½¬æ¢ä¸ºå¹¶è¡Œè®¾è®¡è¿‡ç¨‹(To-be)ã€‚æœ¬æ–‡åº”ç”¨è¯¥æ³•å°†å‘åŠ¨æœºå¤–éƒ¨ç®¡è·¯ç³»ç»Ÿæ•·è®¾è¿‡ç¨‹æ¨¡å‹è¿›è¡Œäº†ä¸²å¹¶è¡Œè½¬æ¢ï¼Œåº”ç”¨å¹¶è¡Œå·¥ç¨‹è¿‡ç¨‹é‡æ„çš„æ‰‹æ®µï¼Œå¾—åˆ°äº†ç®¡è·¯æ•·è®¾å¹¶è¡Œè¿‡ç¨‹æ¨¡å‹ã€‚"
}

instruction = test_texts['instruction']
input_value = test_texts['input']

messages = [
    {"role": "system", "content": f"{instruction}"},
    {"role": "user", "content": f"{input_value}"}
]

response = predict(messages, model, tokenizer)
print(response)


```


## è®­ç»ƒç»“æœæ¼”ç¤º

å¯ä»¥çœ‹åˆ°åœ¨ä¸€äº›æµ‹è¯•æ ·ä¾‹ä¸Šï¼Œå¾®è°ƒåçš„qwen2èƒ½å¤Ÿç»™å‡ºå‡†ç¡®çš„æ–‡æœ¬ç±»å‹ï¼š

![result](./picture/result.png)